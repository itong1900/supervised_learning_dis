{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Set up\n",
    "In this question we compare the effectiveness of different linear classifiers and regression\n",
    "models on the Boston housing and Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "The Boston housing dataset involves predicting average housing prices in an area given\n",
    "features of the area (e.g. average number of rooms per house, average age of owners,\n",
    "etc...). The Iris dataset involves predicting between 3 types of irises given the features\n",
    "of the plant (e.g. petal length, petal width, etc...), however here we only predict on the\n",
    "first two iris types to keep the dataset binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "We begin by loading both the Boston and Iris datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random state for reproducibility.\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "# Load and shuffle the Boston dataset. Only subsample some features.\n",
    "boston_X, boston_y = load_boston(return_X_y=True)\n",
    "permutation = random_state.permutation(boston_X.shape[0])\n",
    "boston_X = boston_X[permutation][:, [5, 6]]\n",
    "boston_y = boston_y[permutation]\n",
    "\n",
    "# Split the dataset into train and test sets.\n",
    "boston_X_train = boston_X[:-100]\n",
    "boston_y_train = boston_y[:-100]\n",
    "boston_X_test = boston_X[-100:]\n",
    "boston_y_test = boston_y[-100:]\n",
    "\n",
    "# Create a new featurization for the boston dataset by turning the current\n",
    "# features into a tenth degree polynomial.\n",
    "boston_poly_X_train = PolynomialFeatures(8).fit_transform(boston_X_train)\n",
    "boston_poly_X_test = PolynomialFeatures(8).fit_transform(boston_X_test)\n",
    "\n",
    "# Now load and shuffle the Iris dataset.\n",
    "# Discarding all output labels that correspond to a 2.\n",
    "iris_X, iris_y = load_iris(return_X_y=True)\n",
    "iris_X = iris_X[:100]\n",
    "iris_y = iris_y[:100]\n",
    "permutation = random_state.permutation(iris_X.shape[0])\n",
    "iris_X = iris_X[permutation]\n",
    "iris_y = iris_y[permutation]\n",
    "\n",
    "# Split the dataset into train and test sets.\n",
    "iris_X_train = iris_X[:-20]\n",
    "iris_y_train = iris_y[:-20]\n",
    "iris_X_test = iris_X[-20:]\n",
    "iris_y_test = iris_y[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Regression and Prediction functions \n",
    "We now define the regression and prediction functions. \n",
    "\n",
    "The squared loss (with regularization) with respect to linear regression is defined as $\\|X\\beta - y\\|_2^2 + \\lambda \\|\\beta\\|_2^2$ where $\\beta$ is the linear model, $X$ is the feature matrix, $\\lambda$ is a regularization term, and $y$ are the true output values. The derivative of $\\|X\\beta - y\\|_2^2$ with respect to $\\beta$ is $2X^{\\top}(X\\beta - y)$ and the derivative of $\\lambda \\|\\beta\\|_2^2$ is $2\\lambda \\beta$.\n",
    "\n",
    "The cross entropy loss with respect to logistic regression is defined as $-\\frac{1}{n} \\sum_{i=1}^n \\left[y_i \\log \\sigma(x_i^{\\top}\\beta) + (1 - y_i)\\log\\sigma(-x_i^{\\top}\\beta)\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_predict(X, beta):\n",
    "    \"\"\"Given a linear model (aka a vector) and a feature matrix\n",
    "    predict the output vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array of shape n\n",
    "        The predicted output vector.\n",
    "    \"\"\"\n",
    "    y = np.dot(X,beta).T\n",
    "    return y\n",
    "\n",
    "def regression_least_squares(X, true_y, lambda_value):\n",
    "    \"\"\"Compute the optimal linear model that minimizes the regularized squared loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vector.\n",
    "    lambda_value : float\n",
    "        A non-negative regularization term.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    beta : numpy array of shape d\n",
    "        The optimal linear model.\n",
    "    \"\"\"\n",
    "    d = X.shape[1]\n",
    "    inverse_part = np.linalg.inv(np.dot(X.T, X) + lambda_value * np.identity(d, dtype = float))\n",
    "    beta = np.dot(np.dot(inverse_part, X.T), true_y) \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have:\n",
    "\n",
    "<div align = \"center\"> $H(y, p) = - \\frac{1}{n} \\sum_{i=1}^n \\big{(} y_i\\log(p_i) + (1-y_i)\\log(1-p_i) \\big{)}$ </div>\n",
    "\n",
    "Finding $p$ that minimizes the cross-entropy loss is the same as finding $p$ that maximizes likelihood:\n",
    "\n",
    "<div align = \"center\">$H(y, p) ~ = ~ - \\frac{1}{n} \\sum_{i=1}^n \\big{(} y_i\\log(p_i) + (1-y_i)\\log(1-p_i) \\big{)} ~ = ~ -\\frac{1}{n} \\mathscr{l}(p \\mid y)$\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_predict(X, beta):\n",
    "    \"\"\"Given a linear model (aka a vector) and a feature matrix\n",
    "    predict the probability of the output label being 1 using logistic\n",
    "    regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array of shape n\n",
    "        The predicted output vector.\n",
    "    \"\"\"\n",
    "    X_beta = np.dot(X, beta)\n",
    "    y = sigmoid(X_beta)\n",
    "    return y\n",
    "\n",
    "def logistic_cross_entropy_loss(X, beta, true_y):\n",
    "    \"\"\"Output the cross entropy loss of a given logistic model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors. Consists of 0s and 1s.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        The value of the loss.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    y_pred = logistic_predict(X, beta)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        if true_y[i] == 1:\n",
    "            total_loss -= math.log(y_pred[i]) \n",
    "        else:\n",
    "            total_loss -= math.log(1-y_pred[i])\n",
    "    \n",
    "    ce_loss = total_loss/n \n",
    "    \n",
    "    return ce_loss\n",
    "\n",
    "def logistic_cross_entropy_loss_gradient(X, beta, true_y):\n",
    "    \"\"\"Output the gradient of the squared loss evaluated with respect to beta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss_gradient : numpy array of shape d\n",
    "        The gradient of the loss evaluated with respect to beta.\n",
    "    \"\"\"\n",
    "    return -np.sum((true_y - sigmoid(X @ beta)) * X.T, axis=1) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, init_beta, true_y, loss, loss_gradient,\n",
    "                     learning_rate, iterations):\n",
    "    \"\"\"Performs gradient descent on a given loss function and\n",
    "    returns the optimized beta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    init_beta : numpy array of shape d\n",
    "        The initial value for the linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors.\n",
    "    loss : function\n",
    "        The loss function we are optimizing.\n",
    "    loss_gradient : function\n",
    "        The gradient function that corresponds to the loss function.\n",
    "    learning_rate : float\n",
    "        The learning rate for gradient descent.\n",
    "    iterations : int\n",
    "        The number of iterations to optimize the loss for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : numpy array of shape d\n",
    "        The optimized beta.\n",
    "    \"\"\"\n",
    "    beta = init_beta\n",
    "\n",
    "    for i in range(iterations):\n",
    "        beta = beta - learning_rate* loss_gradient(X,beta, true_y)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Metrics RMSE\n",
    "For the Boston housing dataset we will use the root mean squared error\n",
    "(RMSE) as our error metric: <br/>\n",
    "<div align = \"center\">RMSE$(\\hat y) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\hat y_i - y_i)^2} = \\frac{1}{\\sqrt{n}}\\| \\hat y - y \\|_2$ </div> \n",
    "where n is the number of datapoints, $\\hat y$ is our predicted price vector and y is the true price vector.<br/>\n",
    "<br/>We'll train a linear regression model by computing:\n",
    "$$\\hat\\beta = \\underset{\\beta}{\\arg\\min} \\|X_{train}\\beta - y_{train} \\|_2^2+ \\lambda\\|\\beta\\|_2^2$$\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, plot the RMSE by using boston_X_train as Xtrain and boston_X_test as Xtest using regularization values $\\lambda \\in \\{0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10\\}$ with raw numerical features without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_pred, true_y):\n",
    "    \"\"\"Given a y_prediction and true_y, calculate the RMSE loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : numpy array\n",
    "        prediction\n",
    "    true_y : numpy array \n",
    "        True y\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rmse: float\n",
    "        RMSE of the given set\n",
    "    \"\"\"\n",
    "    n = len(true_y)\n",
    "    total = np.sum((y_pred - true_y)**2)\n",
    "    rmse = math.sqrt(total/n)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "lambdas = [0, 10**(-4), 10**(-3), 10**(-2), 10**(-1), 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRMSE_lambda(lambdas, train_X, train_y, test_X, test_y):\n",
    "    \"\"\"Given a list of lambdas, and X and true y, return the corresponding RMSE\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lambdas : a list of lambdas as regulization factor\n",
    "    train_X : feature matrix of training set\n",
    "    train_y : true y of train set\n",
    "    test_X  : feature matrix of test set\n",
    "    test_y  : true y of the test set\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rmse_train: list of float\n",
    "        RMSE vs the lambdas on training set, for reference purposes\n",
    "    rmse_test: list of float\n",
    "        RMSE vs the lambdas on test set\n",
    "    \"\"\"\n",
    "    rmse_train = []\n",
    "    rmse_test = []\n",
    "    for l in lambdas:\n",
    "        beta = regression_least_squares(train_X, train_y, l)\n",
    "        y_pred = regression_predict(test_X, beta)\n",
    "        rmse_test.append(RMSE(y_pred, test_y))\n",
    "        y_fit = regression_predict(train_X, beta)\n",
    "        rmse_train.append(RMSE(y_fit, train_y))\n",
    "        \n",
    "    return rmse_test, rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_nonPoly_test, rmse_nonPoly_train = getRMSE_lambda(lambdas,boston_X_train,boston_y_train,boston_X_test,boston_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the RMSE by using boston_poly_X_train as Xtrain and boston_poly_X_test as Xtest use regularization values $\\lambda \\in \\{0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10\\}$. This corresponds\n",
    "to using features $$\\phi(x) = [1, x_1^2,x_1x_2,x_2^2,.....,x_1x_2^4, x_2^5]^T$$\n",
    "where x corresponds to the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_Poly_test, rmse_Poly_train = getRMSE_lambda(lambdas,boston_poly_X_train, boston_y_train,boston_poly_X_test,boston_y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lambdas, rmse_nonPoly_test, label = \"non-Poly-test\")\n",
    "plt.plot(lambdas, rmse_nonPoly_train, label = \"non-poly-train\")\n",
    "plt.plot(lambdas, rmse_Poly_test, label = \"Poly-test\")\n",
    "#plt.plot(lambdas, rmse_Poly_train, label = \"poly-train\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.grid(color='lightgray',linestyle='--')\n",
    "\n",
    "plt.ylim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Recap\n",
    "The regulization doesn't matter too much on the non-poly case, but effectively drops RMSE on the poly case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Metric MAE \n",
    "For the Iris dataset we will use the mean average error (MAE) as our metric:\n",
    "<div align = \"center\"> MAE$(\\hat y) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{1}(\\hat y_i \\neq y_i)$\n",
    "</div> \n",
    "where $\\hat y$ is our vector of 0 − 1 predictions and y is the true vector of labels. <br/><br/>\n",
    "Note that the logistic model outputs a real number $p\\in [0, 1]$, which you need to convert to a 0 − 1 decision. To do so use the following decision function:\n",
    "<div align = \"center\">  $\\delta(p) = \\mathbb{1}(p\\geq 0.5)$\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(y_pred, true_y, threshold = 0.5):\n",
    "    \"\"\"Given y_pred, true_y, return the error rate decided by logistic regression and delta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : numpy array\n",
    "        prediction\n",
    "    true_y : numpy array \n",
    "        True y\n",
    "    threshold: float\n",
    "        The decision threshold, default at 0.5\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    error_rate: float\n",
    "    \"\"\"\n",
    "    n = len(y_pred)\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        if y_pred[i] >= 0.5:\n",
    "            if true_y[i] == 1:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        else:\n",
    "            if true_y[i] == 0:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "                \n",
    "    error_rate = wrong/(correct + wrong)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.arange(start=0.1, stop=1, step=0.1)\n",
    "init_beta = [0,0,0,0]\n",
    "error_rate_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in learning_rates:\n",
    "    beta = gradient_descent(iris_X_train, init_beta, iris_y_train, logistic_cross_entropy_loss, \n",
    "                         logistic_cross_entropy_loss_gradient, l, 100)\n",
    "    y_pred = logistic_predict(iris_X_test, beta)\n",
    "    error_rate_list.append(delta(y_pred, iris_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With learning rate varied from 0.1-1, and iteration = 100, we can reach an MAE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
