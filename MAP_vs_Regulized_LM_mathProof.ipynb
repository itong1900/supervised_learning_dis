{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximize A Posteriori(MAP)  Estimation\n",
    "Here we will consider MAP estimation of regression weights under the linear Gaussian model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, suppose that $x = (x_1, . . . ,x_n)$ are fixed $d$-dimensional vectors, and suppose\n",
    "we obtain observations:<br/><br/>\n",
    "<div align=\"center\"> $y_i = \\beta^Tx_i + \\epsilon_i, i\\in \\{1,2,...n\\}$,\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where as usual $\\epsilon_i \\sim N(0,\\sigma^2)$ are independent and $\\beta \\in \\mathbb{R}^d$ and $\\sigma^2 \\in \\mathbb{R}^+$ are unknown. <br/><br/>\n",
    "We denote $y = (y_1, . . . , y_n)$, and $X$ the matrix with $i^{th}$ row equal to $x_i$. Here we will model\n",
    "the regression weights as a random variable, and place the following prior distribution on them:\n",
    "<div align=\"center\"> $\\beta \\sim N(0, \\sigma_{\\beta}^2 I)$,\n",
    " </div> <br/>\n",
    "In words, this means that every entry of $\\beta$ is distributed as $N(0,\\sigma_{\\beta}^2)$, and that the entries\n",
    "are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The posterior distribution for $\\beta$ after observing the data, $p(\\beta|X, y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\beta|X,y) = \\frac{P(X,y|\\beta)P(\\beta)}{P(X,y)} \\propto P(X,y|\\beta)P(\\beta)$  <br/><br/>\n",
    "$P(X,y|\\beta) = P(y|\\beta;X) = X\\beta+\\epsilon$, since $X, \\beta$ are constant, and only $\\epsilon$ is random. <br/><br/>\n",
    "From the context, $P(\\beta) = \\frac{1}{\\sigma_\\beta\\sqrt{2\\pi}}\\exp(\\frac{-\\beta^T\\beta}{2\\sigma_\\beta^2})$, and $P(\\epsilon) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(\\frac{-\\epsilon^T\\epsilon}{2\\sigma^2})$<br/><br/>\n",
    "Therefore, $P(\\beta|X,y) \\propto  (X\\beta+\\epsilon) \\{ \\frac{1}{\\sigma_\\beta\\sqrt{2\\pi}}\\exp(\\frac{-\\beta^T\\beta}{2\\sigma_\\beta^2})\\} $ <br/> $\\propto \\frac{1}{\\sigma\\sigma_{\\beta} 2\\pi}\\exp(\\frac{-\\epsilon^T\\epsilon}{2\\sigma^2} + \\frac{-\\beta^T\\beta}{2\\sigma^2_\\beta}) $ since $X,\\beta$ are constant<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Now, let's show that the MAP estimator of $\\beta$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"> $\\hat\\beta_{MAP} = \\underset{\\beta}{\\arg\\max}  P(\\beta|X,y)$\n",
    " </div> \n",
    "is also the minimizer of the regularized least squares equation, <br/>\n",
    "<div align=\"center\"> $\\underset{\\beta}{\\arg\\min} \\|X\\beta - y \\|_2^2+ \\lambda\\|\\beta\\|_2^2$\n",
    " </div> \n",
    " for some non-negative $\\lambda \\in \\mathbb{R}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we write $P(\\beta|X,y)$ in a non matrix form, <br/>\n",
    "$P(\\beta|X,y)\\propto \\prod\\limits_{i = 1}^{n}\\exp(\\frac{-\\epsilon_i^2\\sigma_{\\beta}^2 - \\beta_i^2\\sigma^2}{2\\sigma^2\\sigma_{\\beta^2}}) = \\exp(\\sum\\limits_{i=1}^{n}\\frac{-\\epsilon_i^2\\sigma_{\\beta}^2 - \\beta_i^2\\sigma^2}{2\\sigma^2\\sigma_{\\beta^2}}) = \\exp(\\frac{1}{2\\sigma^2\\sigma_{\\beta^2}}\\sum\\limits_{i=1}^{n}(-\\epsilon_i^2\\sigma_{\\beta}^2 - \\beta_i^2\\sigma^2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize $P(\\beta|X, y)$, is the same as maximize $\\log P(\\beta|X, y)$,<br/>\n",
    "$l_1 = log P(\\betaï½œX, y) = -\\frac{1}{2\\sigma^2\\sigma_{\\beta^2}}\\sum\\limits_{i=1}^{n}(\\epsilon_i^2\\sigma_{\\beta}^2 + \\beta_i^2 \\sigma^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximize $l_1$ is the same as minimize $l_2$, and minimize $l_3$. <br/>\n",
    "$l_2 =  \\sum\\limits_{i=1}^n(\\epsilon_i^2\\sigma_{\\beta}^2 + \\beta_i^2 \\sigma^2) = \\sigma_{\\beta}^2\\sum\\limits_{i=1}^n(\\epsilon_i^2+ \\frac{\\sigma^2}{\\sigma_{\\beta}^2}\\beta_i^2)$ <br/>\n",
    "$l_3 = \\sum\\limits_{i=1}^n(\\epsilon_i^2+ \\frac{\\sigma^2}{\\sigma_{\\beta}^2}\\beta_i^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Therefore, $\\underset{\\beta}{\\arg\\min} \\left||X\\beta - y |\\right|_2^2+ \\lambda\\left||\\beta|\\right|_2^2$<br/>\n",
    "in matrix form can be written as $\\underset{\\beta}{\\arg\\min} \\left||\\epsilon |\\right|_2^2 + \\frac{\\sigma^2}{\\sigma_{\\beta^2}}\\left|| \\beta|\\right|_2^2$, <br/>\n",
    "where we treat $\\lambda$ as $\\frac{\\sigma^2}{\\sigma_{\\beta^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Therefore, the regulization term $\\lambda = \\frac{\\sigma^2}{\\sigma_{\\beta^2}}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From what we proved above, we found the equivalence of between MAP estimation and the minimizer of the regularized least squares equation. <br/>\n",
    "- Further, we notice the regulization term $\\lambda$ is equal to the square ratio of error variance and $\\beta$ variance. \n",
    "- From the formula, we can tell, large $\\sigma_{\\beta}$ implies more a smaller regulization, whereas a large $\\sigma_{\\epsilon}$ implies a higher regulization. This intuitively makes sense, because if the variance is mainly comes from the $\\beta$, then regulization doesn't help much since regulization only helps reduce \"overfitting\", by contrast, if the variance mainly comes from $\\epsilon$, then reducing the maginitude of $\\beta$ helps preventing \"overfitting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
